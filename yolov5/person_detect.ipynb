{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbkE9lSq4hOs",
        "outputId": "84eb8ad5-4a26-4c52-c908-347e6d77f62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ v6.1-176-gaa7a0e9 torch 1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 38.2/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pGV2JO4z-Hzy"
      },
      "outputs": [],
      "source": [
        "# create persondet.yaml file in data folder\n",
        "# path: training/data/\n",
        "# train: images/train  \n",
        "# val: images/valid \n",
        "# test:  \n",
        "\n",
        "# nc: 2  \n",
        "# names: ['person', 'person-like']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppxZOSIx7exY",
        "outputId": "71dc8ded-44e9-4280-e563-653c5a2f695e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=persondet.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=2, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m YOLOv5 is out of date by 9 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5  v6.1-167-g488fb0a torch 1.11.0+cpu CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
            "2022-05-06 22:37:26.402468: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
            "2022-05-06 22:37:26.402721: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7025023 parameters, 7025023 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...:   0%|          | 0/944 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...1 found, 0 missing, 0 empty, 0 corrupt:   0%|          | 1/944 [00:09<2:26:57,  9.35s/it]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...10 found, 0 missing, 0 empty, 0 corrupt:   1%|          | 10/944 [00:09<10:42,  1.45it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...21 found, 0 missing, 0 empty, 0 corrupt:   2%|â–         | 21/944 [00:09<04:07,  3.73it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...32 found, 0 missing, 0 empty, 0 corrupt:   3%|â–Ž         | 32/944 [00:09<02:13,  6.81it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...41 found, 0 missing, 0 empty, 0 corrupt:   4%|â–         | 41/944 [00:09<01:33,  9.69it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...49 found, 0 missing, 0 empty, 0 corrupt:   5%|â–Œ         | 49/944 [00:10<01:07, 13.22it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...57 found, 0 missing, 0 empty, 0 corrupt:   6%|â–Œ         | 57/944 [00:10<00:49, 17.75it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...71 found, 0 missing, 0 empty, 0 corrupt:   8%|â–Š         | 71/944 [00:10<00:31, 28.05it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...101 found, 0 missing, 0 empty, 0 corrupt:  11%|â–ˆ         | 101/944 [00:10<00:14, 57.93it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...117 found, 0 missing, 0 empty, 0 corrupt:  12%|â–ˆâ–        | 117/944 [00:10<00:15, 52.54it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...177 found, 0 missing, 0 empty, 0 corrupt:  19%|â–ˆâ–‰        | 177/944 [00:10<00:06, 120.47it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...204 found, 0 missing, 0 empty, 0 corrupt:  22%|â–ˆâ–ˆâ–       | 204/944 [00:11<00:08, 86.63it/s] \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...224 found, 0 missing, 0 empty, 0 corrupt:  24%|â–ˆâ–ˆâ–Ž       | 224/944 [00:12<00:12, 59.75it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...276 found, 0 missing, 0 empty, 0 corrupt:  29%|â–ˆâ–ˆâ–‰       | 276/944 [00:12<00:06, 99.90it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...301 found, 0 missing, 0 empty, 0 corrupt:  32%|â–ˆâ–ˆâ–ˆâ–      | 301/944 [00:12<00:06, 106.35it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...322 found, 0 missing, 0 empty, 0 corrupt:  34%|â–ˆâ–ˆâ–ˆâ–      | 322/944 [00:12<00:05, 108.20it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...341 found, 0 missing, 0 empty, 0 corrupt:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 341/944 [00:12<00:05, 119.20it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...360 found, 0 missing, 0 empty, 0 corrupt:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 360/944 [00:12<00:04, 127.30it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...384 found, 0 missing, 0 empty, 0 corrupt:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 384/944 [00:12<00:04, 124.92it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...415 found, 0 missing, 0 empty, 0 corrupt:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 415/944 [00:13<00:03, 158.83it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...436 found, 0 missing, 0 empty, 0 corrupt:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 436/944 [00:13<00:03, 168.71it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...469 found, 0 missing, 0 empty, 0 corrupt:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 469/944 [00:13<00:02, 196.49it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...498 found, 0 missing, 0 empty, 0 corrupt:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 498/944 [00:13<00:02, 218.24it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...523 found, 0 missing, 0 empty, 0 corrupt:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 523/944 [00:13<00:02, 194.27it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...545 found, 0 missing, 0 empty, 0 corrupt:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 545/944 [00:13<00:02, 175.38it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...565 found, 0 missing, 0 empty, 0 corrupt:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 565/944 [00:14<00:03, 118.31it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...612 found, 0 missing, 0 empty, 0 corrupt:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 612/944 [00:14<00:01, 167.22it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...633 found, 0 missing, 0 empty, 0 corrupt:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 633/944 [00:14<00:01, 164.98it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...653 found, 0 missing, 0 empty, 0 corrupt:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 653/944 [00:14<00:01, 162.63it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...678 found, 0 missing, 0 empty, 0 corrupt:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 678/944 [00:14<00:01, 180.09it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...698 found, 0 missing, 0 empty, 0 corrupt:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 698/944 [00:14<00:01, 173.91it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...724 found, 0 missing, 0 empty, 0 corrupt:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 724/944 [00:14<00:01, 188.71it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...744 found, 0 missing, 0 empty, 0 corrupt:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 744/944 [00:14<00:01, 186.64it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...764 found, 0 missing, 0 empty, 0 corrupt:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 764/944 [00:15<00:01, 149.70it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...781 found, 0 missing, 0 empty, 0 corrupt:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 781/944 [00:15<00:01, 153.92it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...810 found, 0 missing, 0 empty, 0 corrupt:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 810/944 [00:15<00:00, 185.37it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...831 found, 0 missing, 0 empty, 0 corrupt:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 831/944 [00:15<00:00, 169.84it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...850 found, 0 missing, 0 empty, 0 corrupt:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 850/944 [00:15<00:00, 174.63it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...869 found, 0 missing, 0 empty, 0 corrupt:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 869/944 [00:15<00:00, 172.17it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...890 found, 0 missing, 0 empty, 0 corrupt:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 890/944 [00:15<00:00, 133.34it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...933 found, 0 missing, 0 empty, 0 corrupt:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 933/944 [00:16<00:00, 181.53it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train' images and labels...944 found, 0 missing, 0 empty, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 944/944 [00:16<00:00, 58.55it/s] \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\ml\\projects\\person-detection-yolov5\\yolov5\\training\\data\\labels\\train.cache\n",
            "\n",
            "  0%|          | 0/944 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   1%|          | 11/944 [00:00<00:20, 46.20it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):   4%|â–         | 36/944 [00:00<00:18, 49.52it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  12%|â–ˆâ–Ž        | 118/944 [00:00<00:05, 152.13it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  17%|â–ˆâ–‹        | 159/944 [00:01<00:04, 193.59it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram):  20%|â–ˆâ–‰        | 186/944 [00:01<00:08, 87.02it/s] \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram):  22%|â–ˆâ–ˆâ–       | 205/944 [00:02<00:12, 58.62it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram):  23%|â–ˆâ–ˆâ–Ž       | 218/944 [00:03<00:15, 48.13it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram):  24%|â–ˆâ–ˆâ–       | 228/944 [00:03<00:15, 47.47it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram):  30%|â–ˆâ–ˆâ–‰       | 279/944 [00:03<00:07, 88.16it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 312/944 [00:03<00:06, 100.34it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 361/944 [00:03<00:04, 142.96it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 384/944 [00:03<00:03, 153.04it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB ram):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 412/944 [00:04<00:03, 168.65it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB ram):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 469/944 [00:04<00:01, 237.94it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 500/944 [00:04<00:01, 248.90it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 530/944 [00:04<00:01, 229.06it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 557/944 [00:04<00:01, 222.87it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 582/944 [00:04<00:01, 204.86it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.6GB ram):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 616/944 [00:04<00:01, 212.27it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.6GB ram):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 639/944 [00:05<00:01, 201.41it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.6GB ram):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 709/944 [00:05<00:00, 282.03it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 738/944 [00:05<00:00, 237.57it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 763/944 [00:05<00:00, 215.97it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 786/944 [00:06<00:01, 118.56it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB ram):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 831/944 [00:06<00:00, 159.51it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 868/944 [00:06<00:00, 186.31it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 893/944 [00:06<00:00, 141.35it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 919/944 [00:06<00:00, 154.87it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 944/944 [00:06<00:00, 140.00it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\n",
            "    exitcode = _main(fd)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\spawn.py\", line 115, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "MemoryError\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 668, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 563, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 235, in train\n",
            "    shuffle=True)\n",
            "  File \"e:\\ml\\projects\\person-detection-yolov5\\yolov5\\utils\\datasets.py\", line 141, in create_dataloader\n",
            "    collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn), dataset\n",
            "  File \"e:\\ml\\projects\\person-detection-yolov5\\yolov5\\utils\\datasets.py\", line 153, in __init__\n",
            "    self.iterator = super().__iter__()\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 368, in __iter__\n",
            "    return self._get_iterator()\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 314, in _get_iterator\n",
            "    return _MultiProcessingDataLoaderIter(self)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 927, in __init__\n",
            "    w.start()\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\process.py\", line 112, in start\n",
            "    self._popen = self._Popen(self)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\context.py\", line 223, in _Popen\n",
            "    return _default_context.get_context().Process._Popen(process_obj)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
            "    return Popen(process_obj)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
            "    reduction.dump(process_obj, to_child)\n",
            "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
            "    ForkingPickler(file, protocol).dump(obj)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 640 --batch 16 --epochs 1 --data persondet.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXzTHtKFDWUB",
        "outputId": "556248f0-e0ba-48f0-8b87-c5a91fb0c82e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ v6.1-176-gaa7a0e9 torch 1.11.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "# # PyTorch Hub\n",
        "# import torch\n",
        "# from models.common import DetectMultiBackend\n",
        "# from utils.torch_utils import select_device\n",
        "# from utils.augmentations import letterbox\n",
        "# from utils.general import non_max_suppression\n",
        "# import array\n",
        "# import cv2\n",
        "\n",
        "# device = select_device('')\n",
        "# weights = 'runs/train/exp/weights/best.pt'\n",
        "# data = 'data/persondet.yaml'\n",
        "# model = DetectMultiBackend(weights, device=device, dnn=False, data=data, fp16=False)\n",
        "# # model = torch.hub.load(model='runs/train/exp/weights/best.pt')\n",
        "# img0 = cv2.imread('/content/person_dataset/images/train/image (2).jpg')\n",
        "# im = letterbox(img0, 640, stride=32, auto=True)[0]\n",
        "# im = im.transpose((2, 0, 1))[::-1]\n",
        "# # im = np.ascontiguousarray(im)\n",
        "# im = np.array([im])\n",
        "# im = torch.from_numpy(im).to(device).float()\n",
        "# pred = model(im, augment=False, visualize=False)\n",
        "# # # Images\n",
        "# # dir = '../person_dataset/images/test/'\n",
        "# # imgs = [dir + f for f in ('image (1).jpg', 'image (2).jpg')]  # batch of images\n",
        "\n",
        "# # # Inference\n",
        "# # results = model(np.array(imgs))\n",
        "# # results.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5  v6.1-167-g488fb0a torch 1.11.0+cpu CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "1/1: 0...  Success (inf frames 640x480 at 30.00 FPS)\n",
            "\n",
            "0: 480x640 Done. (0.338s)\n",
            "0: 480x640 Done. (0.357s)\n",
            "0: 480x640 Done. (0.312s)\n",
            "0: 480x640 Done. (0.339s)\n",
            "0: 480x640 Done. (0.323s)\n",
            "0: 480x640 Done. (0.403s)\n",
            "0: 480x640 Done. (0.501s)\n",
            "0: 480x640 Done. (0.364s)\n",
            "0: 480x640 Done. (0.364s)\n",
            "0: 480x640 Done. (0.452s)\n",
            "0: 480x640 Done. (0.335s)\n",
            "0: 480x640 Done. (0.360s)\n",
            "0: 480x640 Done. (0.328s)\n",
            "0: 480x640 Done. (0.349s)\n",
            "0: 480x640 Done. (0.312s)\n",
            "0: 480x640 Done. (0.349s)\n",
            "0: 480x640 Done. (0.324s)\n",
            "0: 480x640 Done. (0.333s)\n",
            "0: 480x640 Done. (0.328s)\n",
            "0: 480x640 Done. (0.340s)\n",
            "0: 480x640 Done. (0.329s)\n",
            "0: 480x640 Done. (0.349s)\n",
            "0: 480x640 Done. (0.363s)\n",
            "0: 480x640 Done. (0.354s)\n",
            "0: 480x640 Done. (0.362s)\n",
            "0: 480x640 Done. (0.379s)\n",
            "0: 480x640 Done. (0.368s)\n",
            "0: 480x640 Done. (0.337s)\n",
            "0: 480x640 Done. (0.359s)\n",
            "0: 480x640 Done. (0.330s)\n",
            "0: 480x640 Done. (0.346s)\n",
            "0: 480x640 Done. (0.342s)\n",
            "0: 480x640 Done. (0.347s)\n",
            "0: 480x640 Done. (0.342s)\n",
            "0: 480x640 Done. (0.358s)\n",
            "0: 480x640 Done. (0.353s)\n",
            "0: 480x640 Done. (0.335s)\n",
            "0: 480x640 Done. (0.517s)\n",
            "0: 480x640 Done. (0.513s)\n",
            "0: 480x640 Done. (0.381s)\n",
            "0: 480x640 Done. (0.383s)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_488/3380359765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdetect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'runs/train/person_exp/weights/best.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\ml\\projects\\person-detection-yolov5\\yolov5\\detect.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(weights, source, data, imgsz, conf_thres, iou_thres, max_det, device, view_img, save_txt, save_conf, save_crop, nosave, classes, agnostic_nms, augment, visualize, update, project, name, exist_ok, line_thickness, hide_labels, hide_conf, half, dnn)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# Inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mvisualize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincrement_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mt3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_sync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt3\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\ml\\projects\\person-detection-yolov5\\yolov5\\models\\common.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[1;31m# batch, channel, height, width\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# PyTorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# TorchScript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\ml\\projects\\person-detection-yolov5\\yolov5\\models\\yolo.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# augmented inference, None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# single-scale inference, train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_augment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\ml\\projects\\person-detection-yolov5\\yolov5\\models\\yolo.py\u001b[0m in \u001b[0;36m_forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# save output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32me:\\ml\\projects\\person-detection-yolov5\\yolov5\\models\\common.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 444\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from detect import run\n",
        "weights = 'runs/train/person_exp/weights/best.pt'\n",
        "run(weights, source=0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled23.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
